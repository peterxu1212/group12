\documentclass{amsart}
\usepackage{amsmath}
\usepackage{minted}
\usepackage{fancyvrb}
\usepackage{xcolor}
\usepackage{color}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subfig}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\numberwithin{equation}{section}
\lstset{
	basicstyle=\small\ttfamily,
	columns=flexible,
	breaklines=true
}
\setminted[c]{
	frame=lines,
	framesep=2mm,
	baselinestretch=1,
	fontsize=\footnotesize,
	bgcolor=black,
	style=vim,
	linenos
}
\setlength{\textwidth}{\paperwidth}
\addtolength{\textwidth}{-1in}
\calclayout

\makeatletter
\g@addto@macro{\newpage}{\nointerlineskip}
\makeatother



\begin{document}
\vspace*{-80pt}

\title{IMDB Sentiment Analysis}

\author{Luo, Robin (260851506)}
\author{Rousseau, Marc-Andre  (260089646)}
\author{Xu Bide, (260711367)}

\subjclass[2018]{Comp 551}
\date{\today}
\begin{abstract}
We used ML techniques including logistic regression, naive Baye's and support vector machines to process the data from a large dataset of reviews and train our software to be able to distinguish a favourable review from a negatve one.  In addition to the three methods listed, we used TF-IDF, where the frequency of occurrence of the words within the document and the dataset are both taken into account to ascertain word importance.  Our model was submitted to a kaggle competition and we obtained a score of 0.91786 for the model using a linear SVM, tf-idf.  This is a significant improvement on the Naive Bayes classification method which has an accuracy of approximately 0.83 \end{abstract}
\maketitle
\section{Introduction}(5+sentences)
The ubiquity of social networks is no longer a budding phenomenon and is part of the reality in which we now find ourselves.  Many popular sites have included ways for users to share their opinions on a variety of topics and therefore the ability to mine through these posts and determine how users feel about the things being discussed is extremely useful for businesses.  Knowing that a user or group of users desire something or find it appealing creates a market of opportunity for companies in search of low risk opportunities to expand their business operations.  In addition, the analyses performed, once properly summarized and visualized effectively have tremendous value in themselves.  For our project, we were given 12500 positive and 12500 negative reviews to train our algorithm and another 25000 to test our code and submit our best guess as to the correct labeling of the test reviews as either positive or negative.  Our best model which used a linear SVM (0.918 accuracy), was much better than Naive Bayes (0.83).
\section{Related Work}(4+sentences)
Machine learning and sentiment analysis are hot topics of research with many machine learning conferences having several talks on the topic.  For example, Twitter has been releasing datasets to be mined for things like whether a piece of task is positive, negative or neutral.  Recently, a group of researchers extended this problem to five classification categories and added arabic language content (Rosenthal, 2017).  Many teams submitted ML proposals to classify the twitter posts and the top performing groups used deep neural networks (DNNs) (Rosenthal, 2017).  In addition, out of the top 10 submissions, the second most successful approach to DNNs involved the use of SVMs which is consistent with our best performing model for this project.  A more directly related work involved taking into account sentence negations (Das, 2018).  In this paper, the authors have decided to use a shortcut for negation by negating the word immediately following a negation.  One example of this method would be to take the sentence "I am not happy" which gets converted to "I am not\_happy".  The benefit of this is that by changing only one word, they are able to change the meaning of the entire sentence (Das, 2018).
\section{Dataset and Setup}(3+sentences)
Our training dataset consisted of a list of 25000 reviews properly marked as positive or negative for us to train.  We also had another 25000 data points to test out our model.  The data consist of the content of the reviews and both the training and test sets needed to have their data preprocessed to be able to be consumed by the python libraries.  For the purpose of training and validation, we separated our data into 80\% training and 20\% validation.
\section{Proposed Approach}(7+sentences)
\subsection{Preprocessing}
For our full submitted best model, we began by preprocessing using a combination of custom and library code.  To begin, the data was cleaned of any html tags using a library called Beautiful Soup.  Following this, we converted all of the characters to lowercase as well as escaped some characters which were causing the text processing libraries to error due to conversion problems from unicode to ascii.  In addition to this processing, the data was tokenized and lemmatized.  In addition, a machine learning library was used to standardize some of the symbols with the same meaning but different character encodings.  A regular expression was used to determine whether any emoticons were present and added to our model as a feature.  In addition to this, we isolated punctuation symbols by inserting spaces before and after them.  A negation feature was added and standardized by taking three separate negation patterns and replacing those forms with the word "not" to make negation consistent.   
\subsection{Feature processing}
	In addition to the features generated by the preprocessing, we used tf-idf and bag of words in our pipelines for feature selection.  While we eventually settled on using SVM, we also coded up a Naive Bayes and logistic regression models.  To determine the hyperparameters and for regularization as well as the feature selection, we used a grid-based search algorithm with a cross-validation parameter of 5.  For our bag of words feature, we tested out many different values for possible n-gram size and settled with 1-4 after testing out several different options.  For the SVC model, we tried values for the penalty parameter "c" and settled on the value of 1.  In addition to this, once our model was able to associate good or bad to a particular set of words, we used this word to sentiment mapping to analyse subsentences separated by punctuation.  For each bad or good word found, we added 1 or -1 to a counter and then classified the value for the overall text to the sum of all the individual good/bad words found in the text.  In addition to this, we used a normalization library to standardize the feature vectors so that they had unit length.  In terms of the theoretical underpinnings of the model selection, we used a data-driven approach rather than a conceptual one and so there was little discussion about which model was more appropriate and instead we opted to go with the model that had the best predictive power.
	\begin{itemize}
	\item Discussion of algorithm selection - why did we end up using SVM?  need plots to justify.
	\item Splitting of Data into validation/training, etc...
	\item Regularization strategies (did we use any?  if so, what effect did it have)
	\item Did we use any optimization tricks?
	\item We need plots to justify hyperparameter selection.
	\item Background/motivation for each model (i can do this, but I need to know everything that was done)
\end{itemize}
\section{Results} (7+sentences)


\section{Discussion and Conclusion}(3+ Sentences)
This part can be done last
\section{Division of Work}
\begin{itemize}
\item{\textbf{Robin Luo}}: Model fitting 
\item{\textbf{Marc-Andre Rousseau}}: Literature research, TeXing, some minor coding.
\item{\textbf{Peter Xu}}: Coding, feature selection and hyperparameter fitting.
\end{itemize}
\section{References}
\begin{itemize}
	\item [1] Rosenthal, Sara, et al. “SemEval-2017 task 4: Sentiment analysis in Twitter” Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pp. 502–518.
	\item [2] Das, Bijoyan. Chakraborty, Sarit. "An improved text sentiment classification model using tf-idf and next word negation" June, 2017, eprint arXiv:1806.06407
\end{itemize}
\end{document}
%\begin{figure}[H]
%	\centering
%	\subfloat[MSE vs $\beta$ with 160 word feature vector]{{\includegraphics[width=5.5cm]{msevsbeta160} }}%
%	\qquad
%	\subfloat[MSE vs $\beta$ with 60 word feature vector]{{\includegraphics[width=5cm]{msevsbeta60words} }}
%	\caption{\textbf{A comparison the plots of MSE vs beta for 60 and 160 word feature vectors.}  The other hyperparameters were not constant and needed to be changed since what worked for 60 features did not work for 160.  Despite this, what we can see is a similar behaviour and shape between the two plots but with a difference in the scale.  The rise observed in plot B was also seen in other plots of A where the values were closer to zero than the ones seen in this diagram. }
%	\label{fig:betavsMSE}
%\end{figure}